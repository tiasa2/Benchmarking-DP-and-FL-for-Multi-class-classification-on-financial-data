{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPBERT_Harassment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAC33tfJDZhe",
        "outputId": "89a9240a-a68d-4ca9-dce7-48d570caa5ee"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.96)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxf0OQDeoKQ3",
        "outputId": "947dce2f-7b92-4102-c555-d35033746fae"
      },
      "source": [
        "#!pip install torchcsprng==0.1.3+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install opacus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJAwfU3utMH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import opacus\n",
        "from opacus import privacy_engine\n",
        "\n",
        "\n",
        "# Use GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zscvWxw_3UT-"
      },
      "source": [
        "f = open(\"/content/Sentences_75Agree.txt\", \"r\", encoding=\"ISO-8859-1\")\n",
        "data = f.readlines()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SITGdjen3YHx"
      },
      "source": [
        "text = []\n",
        "labels = []\n",
        "for i in range(0,len(data)):\n",
        "  t = data[i].replace('@',' ')\n",
        "  t = t.replace('\\n','')\n",
        "  text.append((' ').join(t.split()[:-1]))\n",
        "  labels.append(t.split()[-1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyG_0sNe3ZfE"
      },
      "source": [
        "for l in range(0,len(labels)):\n",
        "  if labels[l] == 'negative':\n",
        "    labels[l] = 0\n",
        "  elif labels[l] == 'neutral':\n",
        "    labels[l] = 1\n",
        "  elif labels[l] == 'positive':\n",
        "    labels[l] = 2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS77u8Op3Vj_"
      },
      "source": [
        "df = pd.DataFrame(list(zip(labels, text)),\n",
        "               columns =['Label', 'Text'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8-eJNAz8bA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b13a0e62-0e99-451d-9b7c-a21c03f8e27e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label                                               Text\n",
              "0      1  According to Gran , the company has no plans t...\n",
              "1      2  With the new production plant the company woul...\n",
              "2      2  For the last quarter of 2010 , Componenta 's n...\n",
              "3      2  In the third quarter of 2010 , net sales incre...\n",
              "4      2  Operating profit rose to EUR 13.1 mn from EUR ..."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOV8C2S54FXw"
      },
      "source": [
        "#The class is defined to accept the Dataframe as input and generate tokenized output that is used by the DistilBERT model for training.\n",
        "#The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: ids, attention_mask\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.Text[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Label[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIzvsJpu2UN"
      },
      "source": [
        "#Initialization\n",
        "MAX_LEN = 256\n",
        "VIRTUAL_BATCH_SIZE = 32\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "assert VIRTUAL_BATCH_SIZE % TRAIN_BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / TRAIN_BATCH_SIZE)\n",
        "\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "\n",
        "# Importing BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfPQekmI4JCv",
        "outputId": "37c03fc9-be0a-40da-b3be-797557e15f4e"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3453, 2)\n",
            "TRAIN Dataset: (2762, 2)\n",
            "TEST Dataset: (691, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAr1VZ_vX7B"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # Dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # Relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # Dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # Dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,3)\n",
        "\n",
        "      # Softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      # Pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # Apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oDoBoYdvay8"
      },
      "source": [
        "# Pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# Push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaWy_go71cvg",
        "outputId": "b0bf4414-82e4-4b56-dd32-b68944bae9be"
      },
      "source": [
        "trainable_layers = [model.fc1, model.fc2]\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        total_params += p.numel()\n",
        "\n",
        "for layer in trainable_layers:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "        trainable_params += p.numel()\n",
        "\n",
        "print(f\"Total parameters count: {total_params}\") # ~66M\n",
        "print(f\"Trainable parameters count: {trainable_params}\") # ~0.5M"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters count: 109877507\n",
            "Trainable parameters count: 395267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDNCxGHnvdL1"
      },
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cfy9lZt1vkZ"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tjhQmsGv4_e"
      },
      "source": [
        "SAMPLE_RATE = TRAIN_BATCH_SIZE / len(training_set)\n",
        "LOGGING_INTERVAL = 100 # once every how many steps we run evaluation cycle and report metrics\n",
        "EPSILON = 15\n",
        "DELTA = 1 / len(training_set) # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4GHyK8pqDQx",
        "outputId": "c9d9220a-b63c-424c-c734-b1a1a6a29835"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    target_delta = DELTA,\n",
        "    target_epsilon = EPSILON, \n",
        "    epochs = EPOCHS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:646: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:230: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqP4tgql2fIc"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accuracy(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMwK67-9yfCX"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch, training_loader, testing_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _% 2000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 2000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 2000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        \n",
        "        if (_ + 1) % 2000 == 0 or _ == len(training_loader) - 1:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step()\n",
        "\n",
        "        if _ > 0 and _ % 2000 == 0:\n",
        "              train_loss = np.mean(losses)\n",
        "              eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "\n",
        "              eval_accuracy,eval_loss = valid(model, testing_loader)\n",
        "\n",
        "              print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {_} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"ɛ: {eps:.2f} (α: {alpha})\"\n",
        "              )\n",
        "\n",
        "        \n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux3IS6Kn0W2u",
        "outputId": "fe58d785-5a0e-4d5b-9cbd-4b73c8c493b6"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, training_loader, testing_loader)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 2000 steps: 1.1489380598068237\n",
            "Training Accuracy per 2000 steps: 0.0\n",
            "The Total Accuracy for Epoch 0: 19.188993482983346\n",
            "Training Loss Epoch: 1.1091276996397559\n",
            "Training Accuracy Epoch: 19.188993482983346\n",
            "Training Loss per 2000 steps: 1.0197376012802124\n",
            "Training Accuracy per 2000 steps: 50.0\n",
            "The Total Accuracy for Epoch 1: 19.333816075307748\n",
            "Training Loss Epoch: 1.1085509354668546\n",
            "Training Accuracy Epoch: 19.333816075307748\n",
            "Training Loss per 2000 steps: 1.1450579166412354\n",
            "Training Accuracy per 2000 steps: 25.0\n",
            "The Total Accuracy for Epoch 2: 20.70963070238957\n",
            "Training Loss Epoch: 1.1075167492290452\n",
            "Training Accuracy Epoch: 20.70963070238957\n",
            "Training Loss per 2000 steps: 1.0943536758422852\n",
            "Training Accuracy per 2000 steps: 25.0\n",
            "The Total Accuracy for Epoch 3: 18.718320057929038\n",
            "Training Loss Epoch: 1.1096428598282655\n",
            "Training Accuracy Epoch: 18.718320057929038\n",
            "Training Loss per 2000 steps: 1.081347942352295\n",
            "Training Accuracy per 2000 steps: 0.0\n",
            "The Total Accuracy for Epoch 4: 19.551049963794352\n",
            "Training Loss Epoch: 1.1076193748181955\n",
            "Training Accuracy Epoch: 19.551049963794352\n",
            "Training Loss per 2000 steps: 1.1147722005844116\n",
            "Training Accuracy per 2000 steps: 12.5\n",
            "The Total Accuracy for Epoch 5: 20.92686459087618\n",
            "Training Loss Epoch: 1.1067786371776824\n",
            "Training Accuracy Epoch: 20.92686459087618\n",
            "Training Loss per 2000 steps: 1.0463175773620605\n",
            "Training Accuracy per 2000 steps: 62.5\n",
            "The Total Accuracy for Epoch 6: 20.782041998551772\n",
            "Training Loss Epoch: 1.1060096170171836\n",
            "Training Accuracy Epoch: 20.782041998551772\n",
            "Training Loss per 2000 steps: 1.1232118606567383\n",
            "Training Accuracy per 2000 steps: 12.5\n",
            "The Total Accuracy for Epoch 7: 20.238957277335263\n",
            "Training Loss Epoch: 1.107661298072407\n",
            "Training Accuracy Epoch: 20.238957277335263\n",
            "Training Loss per 2000 steps: 1.0691999197006226\n",
            "Training Accuracy per 2000 steps: 25.0\n",
            "The Total Accuracy for Epoch 8: 19.623461259956553\n",
            "Training Loss Epoch: 1.1079515415119987\n",
            "Training Accuracy Epoch: 19.623461259956553\n",
            "Training Loss per 2000 steps: 1.05890953540802\n",
            "Training Accuracy per 2000 steps: 37.5\n",
            "The Total Accuracy for Epoch 9: 19.37002172338885\n",
            "Training Loss Epoch: 1.1082596553198865\n",
            "Training Accuracy Epoch: 19.37002172338885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CTqOvzy0fVB"
      },
      "source": [
        "#Testing the trained model\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _% 2000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 2000 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 2000 steps: {accu_step}\")\n",
        "\n",
        "            \n",
        "        \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu, epoch_loss\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGP4dZ8d0kOY",
        "outputId": "fd45ae22-704a-420b-febe-54a52922207f"
      },
      "source": [
        "acc, loss = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1it [00:00,  8.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 2000 steps: 1.1249178647994995\n",
            "Validation Accuracy per 2000 steps: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "173it [00:21,  7.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss Epoch: 1.1062316243359118\n",
            "Validation Accuracy Epoch: 13.024602026049203\n",
            "Accuracy on test data = 13.02%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usq1z8_R8mf9"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}